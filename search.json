[
  {
    "objectID": "posts/hierarchical-mmm/index.html",
    "href": "posts/hierarchical-mmm/index.html",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "",
    "text": "Marketing Mix Models (MMMs) often produce one frustrating result:\nYou’ve probably seen it in your own outputs. The big channels look reasonable, but then one smaller campaign — the one that’s been running quietly in the background for months — shows up with a huge uncertainty interval or a coefficient that just doesn’t pass the sniff test.\nIn this post, that channel is LinkedIn Awareness (Always-on): a low-budget paid social campaign that hasn’t had much strategic attention, experimentation, or meaningful budget movement. We’ll walk through why MMMs struggle with channels like this, and how hierarchical modeling can improve measurement by allowing weak channels to “borrow strength” from the rest of the mix."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#business-context",
    "href": "posts/hierarchical-mmm/index.html#business-context",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Business context",
    "text": "Business context\nAssume we work at a DTC subscription brand (think: skincare, supplements, meal kits). Each week we spend across a portfolio of paid social campaigns spanning platforms like Meta, TikTok, Pinterest, Reddit, X (Twitter), and LinkedIn.\nSome campaigns are large and easy to estimate because they have meaningful spend variation and clear signal. Others are always-on and relatively flat.\nIn this example, LinkedIn Awareness (Always-on) is consistently active but has low spend, limited creative rotation, minimal targeting changes, and little week-to-week variation. Its impact may be real, but it’s often subtle and difficult to isolate from everything else happening in the business.\nThis creates a classic MMM failure mode:\n\nThe model is forced to estimate a channel effect using very little usable information."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#a-quick-primer-on-hierarchical-priors",
    "href": "posts/hierarchical-mmm/index.html#a-quick-primer-on-hierarchical-priors",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "A quick primer on hierarchical priors",
    "text": "A quick primer on hierarchical priors\nHierarchical priors are a way of telling the model that some parameters (like campaign effects) are related and should be learned together rather than independently. This allows weak or noisy channels to shrink toward the average effect of similar channels, instead of drifting toward extreme values driven by randomness. This only makes sense when the channels are genuinely comparable. In this post, all campaigns fall under the same broad paid social bucket."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#data-inspection",
    "href": "posts/hierarchical-mmm/index.html#data-inspection",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Data Inspection",
    "text": "Data Inspection\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/arviz/__init__.py:50: FutureWarning: \nArviZ is undergoing a major refactor to improve flexibility and extensibility while maintaining a user-friendly interface.\nSome upcoming changes may be backward incompatible.\nFor details and migration guidance, visit: https://python.arviz.org/en/latest/user_guide/migration_guide.html\n  warn(\n\n\nTo keep this post focused on modeling behaviour (rather than the messiness of real-world attribution), we’ll use a synthetic dataset that resembles a simplified MMM.\nThe data represents a DTC subscription brand with weekly performance data over time. Each week includes an outcome variable (think conversions or revenue) alongside spend across a portfolio of paid social campaigns spanning platforms like Meta, TikTok, Pinterest, Reddit, X (Twitter), and LinkedIn.\nBecause the data is simulated, we know the true underlying coefficients used to generate the outcome. That gives us a rare advantage: we can directly evaluate whether each modeling approach is recovering the ground truth, and how uncertainty changes depending on the assumptions we make.\nA key part of the setup is that LinkedIn Awareness (Always-on) is deliberately designed to be difficult to measure. It has low spend relative to the rest of the portfolio and, more importantly, it shows very little week-to-week variation. This is a common real-world pattern for always-on awareness campaigns, and it’s exactly the kind of situation where MMMs tend to produce unstable or overly uncertain estimates."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#unpooled-approach",
    "href": "posts/hierarchical-mmm/index.html#unpooled-approach",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Unpooled Approach",
    "text": "Unpooled Approach\nWe’ll start with the simplest possible MMM setup: an unpooled model.\nIn an unpooled model, each campaign is treated as having its own completely independent effect size. In other words, the model estimates a separate coefficient for every campaign, without assuming any shared structure or similarity between them. Each campaign’s posterior is learned entirely from the variation in that campaign’s spend and its relationship to the outcome.\nThis is a very common starting point in MMM because it is easy to interpret and aligns with the way many marketers naturally think about performance measurement: each campaign gets its own estimate, based only on its own data.\nIt also provides a useful baseline for comparison. Once we have an unpooled model, we can introduce hierarchy later and clearly see what changes when we allow campaigns to share information through partial pooling.\nThe model is a log-linear regression on the log of spend:\n\\[\\log(y_t) = \\alpha + \\sum_j \\beta_j \\log(x_{j,t}) + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma)\\]\nWith independent priors per channel:\n\\[\\beta_j \\sim \\mathcal{N}(0, 0.1) \\quad \\text{(independent per channel)}\\]\n\nimport pymc as pm\nfrom pymc_extras.prior import Prior\n\ncoords = data[\"coords\"]\nX = data[\"X\"]\ny_obs = data[\"y_obs\"]\n\nwith pm.Model(coords=coords) as unpooled_model:\n    Xd = pm.Data(\"X\", X, dims=(\"date\", \"channel\"))\n    alpha = Prior(\"Normal\", mu=0.5, sigma=0.5).create_variable(\"alpha\")\n    beta = Prior(\"Normal\", mu=0.0, sigma=0.1, dims=\"channel\").create_variable(\"beta\")\n    sigma = Prior(\"HalfNormal\", sigma=0.5).create_variable(\"sigma\")\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(Xd, beta), dims=\"date\")\n    pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_obs, dims=\"date\")\n    pm.Deterministic(\"y_mean\", pm.math.exp(mu), dims=\"date\")\n    pm.Deterministic(\"y_rep\", pm.math.exp(mu + pm.Normal.dist(0.0, sigma)), dims=\"date\")\n\n\nInterpreting the result\nThe forest plot below shows the prior vs posterior distribution of each campaign’s effect size, summarized with a point estimate and a 94% credible interval. In practical terms, this is the MMM’s estimate of how strongly each campaign contributes to the outcome, along with the uncertainty around that estimate.\nFor the larger campaigns, the credible intervals are relatively tight and the posterior is clearly separated from zero. This is what we hope to see: the model is finding enough signal in the data to produce a stable estimate.\nFor smaller campaigns, the intervals tend to widen noticeably. This reflects a simple reality of measurement: if a campaign has limited spend or limited variation over time, the model has fewer opportunities to observe how changes in spend relate to changes in the outcome. The result is a posterior distribution that is much less certain.\nThis is not necessarily a flaw in the model, it is the unpooled approach being honest about what the data can and cannot support. At this stage, the model is doing exactly what we asked: estimating each campaign independently, with no shared assumptions."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#centered-hierarchical-approach",
    "href": "posts/hierarchical-mmm/index.html#centered-hierarchical-approach",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Centered Hierarchical Approach",
    "text": "Centered Hierarchical Approach\nNext, we introduce a hierarchical model using a centered parameterization.\nThe key change is that we no longer treat each campaign coefficient as completely independent. Instead, we assume campaign effects are drawn from a shared distribution with a common mean and spread. In practical terms, this reflects a reasonable marketing belief: campaigns within the same paid social portfolio are different, but they are not totally unrelated.\nThis is often referred to as partial pooling. Campaigns with strong signal are still free to express their own distinct effect sizes, while campaigns with weaker signal are pulled toward the group average. This helps stabilize estimates and reduces the chance that weak campaigns end up with extreme values simply because the model is overfitting noise.\nThe centered parameterization is the most direct way to express this hierarchy, and it provides a useful stepping stone before we move to the non-centered form.\n\\[\\beta_j \\sim \\mathcal{N}(\\mu_\\beta, \\sigma_\\beta)\\] \\[\\mu_\\beta \\sim \\mathcal{N}(0, 0.1), \\quad \\sigma_\\beta \\sim \\text{HalfNormal}(0.1)\\]\nOnly the beta prior block changes:\n# What changed (beta prior block):\nbeta = Prior(\n    \"Normal\",\n    mu=Prior(\"Normal\", mu=0.0, sigma=0.1),\n    sigma=Prior(\"HalfNormal\", sigma=0.1),\n    dims=\"channel\",\n).create_variable(\"beta\")\n\nInterpreting the result\nCompared to the unpooled model, the centered hierarchical model produces noticeably different posterior locations for several campaigns. This is an important observation: by introducing partial pooling, we’ve changed the assumptions of the model, and that often leads to meaningful shifts in where the posterior density sits.\nHowever, the uncertainty for many campaigns remains relatively wide. In other words, the hierarchy has moved estimates toward more plausible values, but it hasn’t dramatically tightened the credible intervals.\nThat’s a useful reminder of what hierarchical modeling can and cannot do. Pooling helps the model make better use of shared structure across campaigns, but it doesn’t magically create new information. If a campaign is consistently low-spend and shows little variation over time, the data still limits how confident the model can be.\nAt this stage, the hierarchy is improving the stability of the estimates more than it is reducing uncertainty. The next step is to reparameterize the same hierarchical model in a way that tends to behave better when the signal is weak."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#non-centered-hierarchical-approach",
    "href": "posts/hierarchical-mmm/index.html#non-centered-hierarchical-approach",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Non-centered Hierarchical Approach",
    "text": "Non-centered Hierarchical Approach\nNext, we fit the same hierarchical model using a non-centered parameterization.\nThis is still the same underlying idea as before: campaign effects are assumed to come from a shared distribution, and weak campaigns are partially pooled toward the group average. What changes is how the hierarchy is expressed, and that can matter depending on both the data and our expectations about how similar campaigns really are.\n\\[z_j \\sim \\mathcal{N}(0, 1), \\quad \\beta_j = \\mu_\\beta + \\sigma_\\beta \\cdot z_j\\]\nOnly the beta prior block changes:\n# What changed (beta prior block):\nbeta = Prior(\n    \"Normal\",\n    mu=Prior(\"Normal\", mu=0.0, sigma=0.1),\n    sigma=Prior(\"HalfNormal\", sigma=0.1),\n    dims=\"channel\",\n    centered=False,\n).create_variable(\"beta\")\n\nInterpreting the result\nThe non-centered hierarchical model typically produces similar overall posterior behaviour to the centered model.\nIn terms of results, we again see meaningful movement in the location of some campaign effects compared to the unpooled baseline. The posterior estimates are more consistent with the idea that these campaigns belong to the same paid social portfolio and should not behave like completely independent outliers unless the data strongly supports it.\nAt the same time, uncertainty remains present for the weakest campaigns. This is expected. Hierarchical modeling improves estimation by borrowing strength, but it does not remove the fundamental limitation that low-variation campaigns provide less identifiable information."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#tuned-non-centered-hierarchical-approach",
    "href": "posts/hierarchical-mmm/index.html#tuned-non-centered-hierarchical-approach",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Tuned Non-centered Hierarchical Approach",
    "text": "Tuned Non-centered Hierarchical Approach\nSo far, we’ve introduced partial pooling and improved the stability of our estimates by treating campaign effects as coming from a shared distribution. In this final step, we add a practical extension: a way to tune the strength of the hierarchy.\nIn many MMM applications, the default hierarchical model learns how much pooling to apply based purely on the data. That is often reasonable, but there are cases where we may want more control. For example, we might believe that certain campaigns should be treated as highly comparable (strong pooling), while others should be allowed more independence (weak pooling).\nTo demonstrate this idea, we introduce a multiplier into the non-centered hierarchy. Conceptually, this gives us a “dial” that controls how much each channel is allowed to deviate from the group-level mean. Smaller values increase shrinkage toward the group mean, while larger values allow more channel-specific freedom.\nThis creates a flexible framework: we still benefit from hierarchical structure, but we also gain a way to encode business judgement about which channels should be pooled tightly and which should be treated more cautiously.\n\\[\\beta_j = \\mu_\\beta + \\sigma_\\beta \\cdot s_j \\cdot z_j\\]\nwhere \\(z_j \\sim \\mathcal{N}(0, 1)\\) and \\(s_j\\) is a channel-specific multiplier that controls how strongly channel \\(j\\) is allowed to deviate from the group mean.\nIntuitively:\n\nIf \\(s_j &lt; 1\\), the channel is more strongly pooled toward \\(\\mu_\\beta\\). The posterior is encouraged to stay closer to the group-level average unless the data provides strong evidence otherwise. This is useful for low-signal channels where we want more conservative estimates.\nIf \\(s_j = 1\\), we recover the standard non-centered hierarchical model. The amount of pooling is driven entirely by the learned group-level variance \\(\\sigma_\\beta\\).\nIf \\(s_j &gt; 1\\), the channel is less pooled (more independent). The model allows that channel’s effect to spread further away from \\(\\mu_\\beta\\), increasing flexibility but also increasing uncertainty. This can be useful when we believe a channel is structurally different from the rest of the group.\n\nIn this example, we might set something like \\(s_{\\text{LinkedIn}} = 0.05\\) to reflect the belief that LinkedIn Awareness should behave similarly to the rest of the paid social portfolio, but is too weakly identified to estimate reliably without stronger shrinkage.\nOnly the beta prior block changes:\n# What changed (beta prior block):\nmu_beta = Prior(\"Normal\", mu=0.0, sigma=0.1).create_variable(\"mu_beta\")\nsigma_beta = Prior(\"HalfNormal\", sigma=0.1).create_variable(\"sigma_beta\")\nz = Prior(\"Normal\", mu=0.0, sigma=1.0, dims=\"channel\").create_variable(\"z\")\nchannels = np.asarray(coords[\"channel\"])\ns_mult = pm.Data(\"s_mult\", np.where(channels == \"c10\", 0.05, 1.0), dims=\"channel\")\nbeta = pm.Deterministic(\"beta\", mu_beta + sigma_beta * s_mult * z, dims=\"channel\")\n\nInterpreting the result\nThe tuned hierarchical model highlights an important practical point: good MMM measurement is often about balancing what the data is saying with what makes sense from a business perspective.\nBy introducing a tuning parameter, we can control how strongly campaign effects are pulled toward the group average. With stronger pooling, the model becomes more conservative and produces more stable estimates that are less likely to swing wildly due to noise. With weaker pooling, the model gives each campaign more freedom to behave independently, but uncertainty typically increases, especially for low-signal campaigns.\nIn this case, tuning has a noticeable impact on the posterior distribution for LinkedIn Awareness (Always-on). This is exactly the kind of channel where the data alone may struggle to provide a clean signal, so the strength of pooling meaningfully shapes the final estimate.\nThis also exposes one of the biggest weaknesses of many “black-box” MMM solutions. If the pooling assumptions are hidden, the model can appear confident or decisive without it being clear whether that confidence comes from the data or from strong structural assumptions baked into the model. Having the ability to tune and inspect pooling behaviour makes the modeling process far more transparent and controllable.\nFrom a measurement perspective, this flexibility is valuable. It gives us a practical way to encode reasonable expectations about campaign similarity, while still allowing the model to surface real differences when the data supports them."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#comparing-all-approaches",
    "href": "posts/hierarchical-mmm/index.html#comparing-all-approaches",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Comparing all approaches",
    "text": "Comparing all approaches\nAt this point we’ve seen four different ways of estimating the same set of campaign effects, using the same underlying dataset. The difference is not the data — it’s the assumptions we allow the model to make.\nThis comparison is where hierarchical modeling becomes most intuitive. The unpooled approach treats every campaign as completely independent, which is a clean baseline but often leads to unstable estimates for smaller or low-variation campaigns. The hierarchical approaches introduce the idea that these campaigns belong to the same paid social portfolio, and therefore their effects should live in a similar range unless the data strongly suggests otherwise.\nThe plot below summarizes the key takeaway: even though the credible intervals may remain wide for difficult-to-measure campaigns, the hierarchical approaches tend to produce posteriors that are more stable, more realistic, and more aligned with how marketers actually think about campaign performance. Rather than allowing weak channels to drift toward extreme values, the hierarchy pulls them back toward the broader pattern of the portfolio.\nThis is also a useful reminder that MMM results are never purely “what the data says.” They are always a combination of data and assumptions. Comparing these models side-by-side makes those assumptions visible, and helps clarify which approach produces estimates that are most useful for decision-making.\nIn practice, the goal is not to eliminate uncertainty — it’s to make uncertainty behave sensibly. A good MMM should produce outputs that are credible enough to act on, while still being honest about where the data is weak.\n\n\n\n\n\n\n\n\n\nQuantitative summary\nTo complement the visual comparison, we can also summarize the results numerically. The table below reports the point estimate for each approach, the percentage difference between that estimate and the known true value, and whether the true value falls inside the credible interval (coverage). This provides a simple way to compare accuracy and calibration across the different modeling assumptions.\n\n\n\n\n\n\n\n\n\nModel\nPoint estimate\nTrue value\nPoint est. % diff\nCoverage\n\n\n\n\n0\nUnpooled\n0.0901\n0.4573\n-80.29%\nNo\n\n\n1\nCentered hierarchy\n0.4312\n0.4573\n-5.71%\nYes\n\n\n2\nNon-centered hierarchy\n0.4344\n0.4573\n-5.01%\nYes\n\n\n3\nNon-centered + tuned pooling\n0.4535\n0.4573\n-0.84%\nYes\n\n\n\n\n\n\n\n\nCoverage indicates whether the 94% HDI contains the true value."
  },
  {
    "objectID": "posts/hierarchical-mmm/index.html#conclusion",
    "href": "posts/hierarchical-mmm/index.html#conclusion",
    "title": "Improving Marketing Measurement with Hierarchical MMMs",
    "section": "Conclusion",
    "text": "Conclusion\nThis example highlights a common reality in MMM: some campaigns are simply hard to measure in isolation. Always-on awareness spend, low-budget channels, and low-variation campaigns often contain too little identifiable signal for an unpooled model to produce stable, decision-ready estimates. In those cases, the model isn’t necessarily “wrong” — it’s being honest about what the data can support.\nHierarchical modeling offers a practical way forward. By treating campaigns as part of a shared portfolio, we can borrow strength across similar channels and produce estimates that are often more stable and more aligned with business intuition, without pretending uncertainty doesn’t exist. The flexibility to tune pooling strength is especially important, and it exposes a major weakness of many black-box MMM solutions: if the structural assumptions are hidden, it becomes unclear whether confidence is coming from the data or from aggressive shrinkage baked into the model. The good news is that hierarchical approaches are relatively straightforward to implement in PyMC, and they provide a transparent framework for balancing data-driven insights with sensible modeling assumptions grounded in how marketing actually works.\nKey take-home: MMM isn’t just about fitting the data: it’s about combining evidence with reasonable business assumptions, and hierarchical priors are one of the most practical tools for making weak channel measurement more stable and usable."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jake Piekarski",
    "section": "",
    "text": "This site hosts MMM and Bayesian modelling related blogs written by me. Here you’ll find practical guides, case studies, and technical deep-dives on Marketing Mix Modelling, hierarchical priors, incrementality testing, and causal inference—all grounded in Bayesian statistics and implemented in PyMC. Head to Blogs to browse all posts."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jake Piekarski",
    "section": "Bio",
    "text": "Bio\nI’m a data scientist with a passion for Bayesian statistics and marketing measurement. Currently working as a Marketing Data Scientist, I specialise in advanced statistical modelling, including Marketing Mix Modelling (MMM), incrementality testing, and causal inference. My expertise lies in translating complex data into actionable insights to optimise marketing strategies and fuel data-driven decision-making. I have both agency and consultancy experience, working with a wide range of clients across various industries—from airlines to Fortune 100 CPGs.\n\n\n\nEducational Achievement\nObtained From\nClass\n\n\n\n\nBachelor’s in Mathematics\nUniversity of Nottingham\nFirst class\n\n\nMaster’s in Mathematics\nUniversity of Nottingham\nFirst class"
  },
  {
    "objectID": "index.html#pymc-labs",
    "href": "index.html#pymc-labs",
    "title": "Jake Piekarski",
    "section": "PyMC Labs",
    "text": "PyMC Labs\nI’m currently a Data Scientist at PyMC Labs, the Bayesian AI consultancy founded by the creators of PyMC. PyMC Labs partners with organisations worldwide to design, implement, and optimise bespoke Bayesian models—from Media Mix Modelling and marketing analytics to AI systems and causal inference. I joined the team in January 2025 and contribute to client projects and open-source tooling in the Bayesian marketing and analytics space."
  },
  {
    "objectID": "index.html#jtp-analytics",
    "href": "index.html#jtp-analytics",
    "title": "Jake Piekarski",
    "section": "JTP Analytics",
    "text": "JTP Analytics\nJTP Analytics is my own consultancy, offering expertise in Marketing Mix Modelling and marketing measurement. Whether you need model development, validation, or support with incrementality testing and attribution, I help teams turn complex data into clear, actionable insights. Reach out at jakepiekarski@jtp-analytics.com for any MMM or marketing analytics support.\n\nThank you for reading."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Title\nLink\n\n\n\n\nImproving Your Marketing Measurement with Hierarchical MMMs\nRead →"
  }
]